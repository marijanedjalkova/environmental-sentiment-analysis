\let\textcircled=\pgftextcircled
\chapter{Design \& Implementation}
\label{chap:des_imp}

\initial{T}his chapter will discuss in detail the design decisions made in this project, and any specifics of the implementation phase. 
The whole process of the application could be divided into several stages and described using the figure \ref{fig:workflow}. The tweets are taken from Twitter using Twitter API, after which they are passed through two models: a sentiment analysis model and a topic modelling model. Two questions are asked: ``Is a tweet positive or negative?'' and ``Does the tweet talk about a certain topic?''. If at least one of the answers is ``No'' then the tweet is discarded. Otherwise it is marked as being relevant.

The project has been written entirely in Python because most of the existing Natural Language Processing tools, especially the NLTK library, are implemented in 

The rest of the chapter will discuss in detail both of the models and the design decisions behind them. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{workflow}
    \caption{General Workflow}
    \label{fig:workflow}
\end{figure}

%=======
\section{Existing Tools}
\label{sec:tools}

The main constraint of this project has been the time limitation. A lot of small steps involved in the processing of data could have been separate study fields, and an immense amount of research could have been potentially devoted to these fields. Because this project is limited in time, for a lot of these cases, only the existing libraries could be used for some techniques. Some of such methods are imperfect and could be improved. This will be discussed in the conclusions chapter in greater detail, but in this chapter it will be mentioned a lot that certain trade-offs had been made to make the project possible. In most cases, existing libraries have been at least tried because, mostly, it is complicated and time-consuming to implement techniques related to Natural Language Processing from scratch, and it also often requires large data sets. 

\paragraph{Twitter API}\mbox{}\\
For the first stage of the project, tweets were obtained from Twitter using Twitter API. The keyword used for search was ``Grangemouth''. Ideally, it would be better to locate tweets geographically and select the ones coming from the Grangemouth area. This would allow to also include tweets that do not mention the area but talk about it. However, as it was discussed in the Context Survey chapter, it is quite complicated to extract the geographical data out of the tweets. Most of them do not contain coordinates, and the heuristics which is widely used in research is not ethical or accurate. This is why it was decided to only focus on the tweets found through search of the ``Grangemouth'' keyword. Streaming of live tweets was also implemented using the same API.

\paragraph{Natural Language Processing Libraries}\mbox{}\\
NLTK\footnote{\url{http://www.nltk.org/}}, the Natural Language Toolkit, is the most well-known platform for Natural Language Processing. It provides access to a variety of methods used to process text and has interfaces which help access some of the largest corpora such as WordNet.

TextBlob\footnote{\url{https://textblob.readthedocs.io/en/dev/}} is a Python library for processing textual data. Although it is often seen as an alternative to NLTK, in come cases it builds on the functionality provided by the NLTK. 

Both of these libraries have been used extensively for different purposes in the project such as noun phrase extraction, text tokenisation, stemming, part-of-speech tagging, parsing, document classification and other. 

Another library used is the preprocessor\footnote{\url{https://github.com/s/preprocessor}}. It is a library for tweet preprocessing in Python and helps extract Twitter-specific entities such as emojis, URLs, and hashtags.

%=======
\section{Sentiment Analysis}
\label{sec:sa}

\subsection{Training datasets}

For sentiment analysis, a large dataset is required to training the statistical model. Creating such a dataset is a hard and time consuming task not only because one would have to go through a large amount of data but also because people do not always agree on the polarity of a sentence. Hence, most of the researchers use several well-known datasets with tagged data. 

In addition, performing sentiment analysis on Twitter data is a specific task, and a model trained on, say, Shakespeare texts could not be used on Twitter data. So, the dataset had to consist of tweets.

Probably, the largest dataset available publicly and referenced on the The World Wide Web Consortium website \footnote{\url{https://www.w3.org/community/sentiment/wiki/Datasets}} is the Thinknook dataset. It consists of 1,578,627 classified tweets, where each tweet is tagged by 0 if it is negative or 1 if it is positive. The advantages of the dataset are, apart from its size, the fact that the data is shuffled within the dataset, so the researcher would not have to put any effort into making sure that when training or testing data is selected from the dataset, approximately half of the tweets are positive and half are negative. However, at the same time, closer look and the first tests showed that, in fact, the dataset consists of the tweets taken from two different sources: Sentiment140 dataset and Kaggle dataset, and the Kaggle dataset only contains tweets about movie reviews, which would make the model biased and not work properly on data related to life in a Scottish town or environmental issues. So, it was decided to only focus on the Sentiment140 dataset.

Sentiment140 dataset \footnote{\url{http://help.sentiment140.com/home}} is one of the most well-known Twitter sentiment analysis datasets and was created by several Stanford University graduates. In the dataset, which is available for access, tweets have several fields and are tagged as positive (4) or negative (0). This dataset, however, is not perfect. For example, the creators say that the data was not labelled manually but automatically. This is usually fine, except, in this case, the authors say that the criteria for automatic tagging has been the smileys: ``we assume that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative'' \cite{sentiment140}. There are several big issues with this approach. First, a smiley can be deceiving or used ironically, as a sarcasm. Secondly, Twitter currently supports emojis, which are complicated Unicode characters, so the heuristics chosen by the researchers would not be efficient. Also, even though roughly half of the tweets in the dataset are positive and half --- negative, the tweets are not shuffled, which means that they had to be shuffled initially and extra care had to be taken to ensure that the training and testing sets had roughly equally spread data in them.

In addition, the same authors also created an online tool that accepts a tweet text request and returns a response with the polarity of the text. Presumably, this tool also uses a model created from the same dataset, so it would make sense to, when validating a newly created model, compare its responses to the ones provided by that model. However, this is impossible because, unlike the dataset, which uses only two possible labels: ``positive'' and ``negative'', the online model adds a third label - ``neutral''. This makes it impossible to compare the two models. 

Unfortunately, the rest of the datasets available would either not be large enough, or not focus specifically on Twitter data, or have other flaws, so the Sentiment140 dataset seemed to be the best one of all the available ones. 

When the main objective is to work with the human-created data, it is not easy to generalise the behaviour of different language models. A model might work well on some data but perform much worse on another data set. That is why throughout this project, the evaluation of the model consisted not only of the statistical evaluation based on testing data but also of manual evaluation based on the `live' data coming directly from Twitter. When creating the model, the main ideas have been based on the results of the most recent research, which has been discussed in the Context Survey chapter, however, in some cases, certain changes have been made due to, for example, better performance of such modified models. Overall, the workflow of the model creation has been following an Expectation-Maximisation process: a model was created, tested, evaluated, and then the loop would start again, with modifications.

\subsection{Feature Vector Extraction}
In order to train a classifier, the input data has to be represented as a feature vector. The success of the entire model depends on the structure of the vector, and it is up to the researcher to design it. In Python, a feature vector is usually a dictionary where the keys are the names of the properties and the values are usually numeric or Boolean values of those properties. For example, it could look like this: \\ \texttt{\{`Length':139, `hasHashtags':True, `containsNegativeWords':False\}}. \\Alternatively, a typical unigram model will look somewhat like this:\\ \texttt{\{`I':True, `am':True, `happy':True, `today':True\}}.\\Nothe that while in the first case, the vector will always be of the constant size, the latter structure will have a variable number of key-value pairs. 

 A lot of models created for research working with the natural language data use the unigram model \cite{agarwal2011sentiment}. It performs well when the researcher initially does not have a lot of knowledge about the training data, or when it is too vast (like anything concerning natural language) to be summarised in several fields. Also, there is not enough research looking into how different structures of the feature vector affect the performance of the sentiment analysis model and how to best design a feature vector for an input consisting of natural language. It would be interesting to look at this problem, but it is a case for the future studies and does not lie in the scope of this project. In this project, the unigram model is the basis model of the feature vector.  

\paragraph{Tokenisation}\mbox{}\\
The first step of feature extraction os tokenisation. Tokens are `units' of text that the input string is being split into; they are most often words but could also contain URLs, emojis, hashtags, or other Twitter-specific data. The initial simple manual tokenisation showed that simply tokenising text using NLTK provided default methods would not work particularly well. The figure \ref{fig:manual_tok} shows the pairs of tuples, where the first item is the token, and the second item is a number obtained by adding 1 every time it appeared in a positive tweet or subtracting 1 every time the token appeared in a negative tweet. The figure shows the first and the last twenty items of an entire list. Note that the results are not normalised, since it was a small bit of the initial exploratory analysis, and the goal was simply to see what the data looks like, in general. As it can be seen in figure \ref{fig:manual_tok}, most of the most popular tokens are not words but rather punctuation or the `@' character which most often refers to a mention in Twitter. At the same time, this initial simple analysis showed some really important facts about the dataset which would allow make an educated guess about the data. For example, we can say that the word ``love'' is predominantly used in positive tweets, together with words ``great'' and ``you''. In addition, the `\#' sign is also in the first half of the image, which could mean that, overall, hashtags are more popular in the positive tweets.

Predictably enough, the list of token appearing mostly in negative tweets mentions the words ``work'', ``sad'' and ``miss''. It could be surprising for some (or explained in quite a philosophical manner) that this list also contains the pronoun ``I'' in three different forms. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{most_pos_neg}
    \caption{Tokens that appear in most positive and most negative tweets, default tokeniser}
    \label{fig:manual_tok}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{most_pos_neg_tw_tokenizer}
    \caption{Tokens that appear in most positive and most negative tweets, TweetTokenizer}
    \label{fig:tweettok}
\end{figure}


Thankfully, NLTK also provides a \texttt{TweetTokenizer} which, unlike the default tokeniser, would, for example, see ``@marianna'' as one token and not two different ones. Figure \ref{fig:tweettok} shows the result of the same analysis as in \ref{fig:manual_tok}, but using \texttt{TweetTokenizer}. It can be seen that the list has changed significantly, even though ``I'' and ``i'' are still the tokens that are mostly seen in negative tweets (at this point of the initial analysis, the tokeniser was case-sensitive). 


\paragraph{Pre-processing} \mbox{} \\
Due to the nature of natural language, before any features can be extracted from the input data, it has to be processed. Human-produced data is non-structured, and using the vast amount of unstructured data has been one of the largest problems for data science. Pre-processing data can help make it more structured on later stages of feature extraction. 

Pre-processing natural language input usually involves dealing with such problems as different in upper or lower cases, usage of stop words, abbreviations, any other structures. In the case of this project, it has to be remembered that Twitter has certain structures not normally used in other texts, such as URLs, mentions, hashtags. For the ease of work with different words rather than individual symbols, the pre-processing can be performed immediately after tokenisation. 

For this project, \texttt{ngram\_classifier.NGramClassifier.preprocessing\_extractor()} does all the preprocessing. 

First, the tweet often would have to be decoded. One of the hardest tasks of this project was to keep the data consistent. The data was coming from different sources such as text files of different formats, as well as the live Twitter feed. Some of the input was represented with strings, whereas in other cases it would be in Unicode. The text had to be encoded, and the potential Unicode decode and encode errors had to be accounted for. In some cases, the tweet would not be encoded successfully because it would contain characters that do not exist in English language and could not possibly be translated to ASCII. In cases where no sense could be made out of a tweet, the tweet would have to be skipped. 
\paragraph{}
The \texttt{preprocessor} library was used to extract URLs, mentions, emojis and hashtags from a tweet. For \textbf{URLs}, if there were any in a tweet, they would be each summarised to a single dictionary entry with a `True' value (\texttt{\{``\$URL\$'':True\}}. Most tweets contain at most one external link, so it made sense for the URLs to be summarised to a Boolean value. Deeper research might be able to detect how different links affect the polarity of a tweet. This could potentially be done by accessing the data from the link and trying to determine the polarity of that data, or, alternatively, maybe certain amount of polarity could have been extracted from the domain name itself. However, this is also a case for a further study and is not in the scope of this project. 
\paragraph{}
With the \textbf{mentions}, the situation is not so clear, so the approach taken is just one of a number of possibilities. Similarly to the links, all the mentions are summarised into a dictionary entry of the form \texttt{\{``\$MENTION\$'':True\}}). Alternatively, it was possible to record the number of mentions used in every tweet. However, that number would still be quite low and probably wouldn't affect the resulting model much. Some might argue that it would be useful to use the mention as a source of information about polarity since a mention is a `@' character followed by the username of a user being mentioned, however, it would not be reasonable to assume that each Twitter user is either happy or negative the whole time, even though some people are generally more positive than the others. Overall, this is yet another field of study and is also not in the scope of this project.
\paragraph{}
Unlike the URLs and the mentions, \textbf{hashtags} (`\#' followed by a word or a combination of words that are not separated by any form of whitespace) actually contain text information which could be interpreted in several ways. One of the approaches would be to try and parse the hashtags into the actual text. This would involve quite complicated parsing. Let's look at a sample hashtag, `\#teamGBolympic'. When parsing this hashtag, the desired output would have been the following list: \\ \texttt{[``team'', ``GB'', ``olympic'']}.\\ After that, another bit of processing would have to determine that ``GB'' stands for ``Great Britain''. The result of such parsing would provide data which could be added to the other words and interpreted as additional input for the model. 

On the other hand, it is possible to argue that hashtags are not simply combinations of words. They usually define a concept in a very concise way and are used by a large number of people, becoming trends and being passed between people. If the concept was not trendy and Twitter did not have support for hashtags, the same exact concept could have been expressed in a large number of ways, like almost all concepts in natural language. Yet this specific choice of words and, sometimes, characters, makes hashtags more than words. For that reason, it seemed more sensible to record the hashtag as it is instead of trying to parse it into a list of words. 

\paragraph{}
Finally, the \textbf{emojis} were processed separately. Unlike emoticons, which are simple combinations of text characters, emojis are 12-by-12 pixel images. When received from Twitter as a part from text, they are represented by Unicode sequence of characters. Every emoji has its own unique Unicode codepoint, but the emojis are not sorted in any way which would allow to extract any meaningful information from the code alone. At the same time, each emoji clearly carries a lot of information about sentiment. 

Fortunately, some research has been done to detect the polarity of each emoji. An online ranking exists where each emoji has been given a sentiment score from -1 to 1. The ``Sentiment of Emojis'' paper describes the construction and analysis of the ranking \cite{Kralj2015emojis}. Unfortunately, the ranking only exists as a web page and has no API, so in order to use it, it was necessary to scrape the page. The \texttt{BeautifulSoup} library allows parsing the HTML received from a URL and extract information from the DOM of the page. So, the information from the web page has been scraped and saved locally as a CSV file which contains a list of a format \texttt{"0x1f44f": 0.52}, where the key is the unique codepoint of the emoji and the value is its sentiment score. This allows to avoid scanning the page every time the application is used. If at any point the file cannot be found, the page will be scraped again automatically, otherwise the local file will be used. 

When an emoji is encountered in the tweet, it is looked up in the local file and its sentiment score is extracted. If the emoji is not found (the `vocabulary' of emojis keeps developing), it is being skipped. The sentiment scores of all emojis are summed up into one number and added to the feature vector in the following format: \texttt{"\$EMOJI\$":0.42}. 

\paragraph{}
After all the pre-processing all the URLs, emojis, hashtags and mentions are cut out of the text. The rest of the string is tokenised using \texttt{TweetTokenizer}. All of the tokens are converted to \textbf{lower case}, apart from the cases where the entire word is in upper case. This is just one approach and the could have been done in another way, of course. Usually it is useful to have all the text in one case (say, lowercase), since it, for example, makes ``Today'' (first word in the sentence) be equal to ``today'' (a word in the middle of a sentence) and ultimately improves the statistical model. However, when the data is as informal as it can be on Twitter, it might sometimes be useful to save the case of the words. For example, if the whole sentence is in uppercase, it might mean that the sentence expresses string emotions, and it can help detect polarity of the sentence. 

\paragraph{}
Finally, the resulting list of token is stripped off the so-called \textbf{stop words} and the punctuation. 
The punctuation, however, does not seem to affect the performance of the model at all. Stop words are the non-significant words that, for example, would not be used in a Google query. They act as a glue for the rest of the words in the language. For example, nouns and most adjectives would not be considered stop words, but  articles and conjunctions would be classified as stop words. In a lot of applications related to Natural Language Processing, including sentiment analysis, removing stop words is considered a good practice \cite{poursepanj2013uottawa, kouloumpis2011twitter}. However, experiments showed that the model where the stop words are stripped performs slightly worse comparing to the model where the stop words remained. Research suggests that in some cases, certain stop words are useful for defining sentiment of a text \cite{lin2009joint}. 


\paragraph{Noun Phrase Extraction}\mbox{}\\
Further inquiry into the results of the existing sentiment analysis research opened another topic for discussion. The initial versions of the classifier were working with individual words. However, in natural language, meaning (and, thus, polarity) of individual words often depends on the context. Because of this nature of the natural language, it would be more beneficial to work with phrases rather than individual words. 

Phrase extraction is, however, quite a complicated task since the term ``phrase'' could not be defined formally. In natural language, phrases take different forms, have different lengths and structures. Noun phrase extraction is an important task in Natural Language Processing. The NLTK book \footnote{\url{http://www.nltk.org/book/ch07.html}} describes a way of the noun phrase chunking based on a context-free grammar. The context-free grammar can be defined in a number of ways, and the authors of the book define quite a simple one in the example \footnote{\url{http://www.nltk.org/book/pylisting/code_chunkex.py}}: 
an optional determiner, followed by zero or more adjectives, followed by a noun. 

The grammar looks as follows: 

\texttt{grammar = "NP: {<DT>?<JJ>*<NN>}"}. 

Writing a context-free grammar that would cover all of different forms of noun phrases is a complicated task. In addition, in order to use such a grammar, it is necessary to know the parts of speech of every word. Even though NLTK provides access to large corpora of text, including online chat texts, it is unreasonable to expect every word mentioned on Twitter to be known to NLTK. It would require tagging manually a large amount of text, including abbreviations and slang. In addition, it would require performing error correction on the data because it is unreasonable to expect that the tweets contain to typos or grammatical errors. Overall, it seemed that implementing a noun phrase extractor from scratch could not possibly fit into the scope of the current project.

The TextBlob library provides two methods allowing automatic noun phrase extraction: \texttt{Fast\_NP\_extractor} and \texttt{ConllExtractor}. The second extractor ``uses the CoNLL 2000 corpus to train a tagger'' \footnote{\url{http://textblob.readthedocs.io/en/dev/advanced_usage.html}}. However, neither of the extractors performed on a good level. Both would skip a lot of important noun phrases, find at most one noun phrase in the given tweets, and take around three seconds to extract noun phrases from one tweet. For example, from ``what a lovely day it is right now, going for a walk. I hope to see a lot of sun'' only ``lovely day'' and ``what lovely day'' are being extracted by each of them correspondingly. Even if we assume that the extractor only accepts a sentence at once, both of the extractors still skipped the ``a walk'' phrase.

In a situation where a model has to be trained on tens of thousands of tweets, such poor performance would not be acceptable, and, unfortunately, it was impossible to use noun phrase extraction successfully as a part of this project. 

\paragraph{N-Grams}\mbox{}\\
The existing research and different tests show that using n-grams might give as good results as noun phrase extraction, with much less computation. In fact, some papers on Twitter sentiment analysis mention that unigram model outperforms the bigram or trigram model \cite{agarwal2011sentiment}. Yet, because it seemed counter-intuitive, bigram and trigram models were also tested.

Using NLTK library, it is possible to see the ``most definitive features'' of the model once it is trained. The most definitive bigrams are shown in figure \ref{fig:bigrams}. Each row in this figure shows a feature and how it affects the result given by the classifier. For example, the first row shows that, when the input text contains the combination of tokens ``100'' and  ``followers'', then it makes the tweet positive (``1:0'' shows which label of the two would more likely to be chosen) with the probability 57.2:1.0, which is quite a large proportion. The items in the list are sorted by this proportion, so 57.2 is the largest proportion in the entire sample set.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{most_definitive_bigrams}
    \caption{Most definitive bigrams, using Naive Bayes model and TweetTokenizer}
    \label{fig:bigrams}
\end{figure}

The bigram model would, indeed, show slightly lower accuracy than the initial model, where the classifier would use default methods for obtaining unigrams, even though it would still be above 70\% for most of the parameters. However, it was much faster and could be trained on the amount of data that was several times higher than the data that could feasibly be used by the initial model. 

With trigrams, the accuracy dropped to around 60\%, proving the reference made in the paper of Agarwal et. al. that trigrams perform worse than bigrams and unigrams \cite{agarwal2011sentiment}. The model would still be able to train on large amounts of data in a matter of seconds. With Twitter, it could probably be argued that trigrams are too long as constructs to act as meaningful phrases. In a text full of special symbols, mentions, abbreviations, slang and links, and in a text that is also less than 140 characters long, three words will probably have quite a lot of content squeezed into them. Another issue is that removing the so-called ``stop words'' is considered a good technique in sentiment analysis. Stop words are usually short words that do not carry a lot of meaning. When stripped of these words, the text loses its fluency, so the three tokens taken in a row might not even carry any meaning. Another issues is that, statistically, chances of seeing the same trigram several times even in a large dataset are usually quite low, which makes it hard to train a good statistical model.

It is important to note that, for all the experiments with n-grams, NLTK method \\ \texttt{ngrams(input, n)} was used. In accepts the input string and n, which defines how many tokens there are going to be in one n-gram. An n-gram is a tuple of length n. For example, when \texttt{n = 2}, the output list would have items of a format \texttt{("alpha", "beta")}, which is quite predictable. When \texttt{n = 1}, the method still outputs a list of tuples, even though each tuple will only contain one element and, if printed out, will have a format \texttt{("alpha",)} (by putting the comma after the only element, Python specifies that it is, in fact, a tuple). 

When the \texttt{ngrams()} method was used to obtain unigrams, surprisingly enough, the resulting model training was also much faster and memory-efficient than when the default methods were used initially. It could probably be explained by the inefficiency of the NLTK library methods splitting the input. So, this unigram model was used for all the consequent experiments with different classifiers. 

\subsection{Classifiers}
Since the classifiers have already been implemented in the NLTK, it provided an opportunity to test the behaviour of the same dataset on different classifiers and compare them, with minimal effort and time. 
\paragraph{TextBlob polarity} \mbox{}\\
The first model that has been tried for the sentiment analysis was the TextBlob's sentiment analysis model which can be accessed using the \texttt{polarity()} method, which returns -1 if the text is negative and 1 if it is classified as positive. It is an example of a knowledge-based classifier. The results given by the classifier were not very high. When tested on the Sentiment140 dataset, the accuracy of the classifier was 60.04\%. When working with live untagged data, it could be clearly seen that, in a lot of cases, the model would disagree with a general human opinion. The rest of the models created and tested are statistical. 

\paragraph{Naive Bayes Classifier}\mbox{}\\
In most of the papers used as a basis for this project and referenced in the Context Survey, Naive Bayes classifier is mentioned as a very simple classifier that is rarely expected to perform well due to the independence assumptions made, but also as a classifier that often performs almost as good as the state-of-art classifiers.

Indeed, the initial model proved to be quite successful. At different stages of the project, its accuracy would be between 53\% and 88\%.

The initial versions of the classifier were very slow to train. In these versions, a classifier would accept a tweet as a string and use default methods to tokenise the tweet and perform some pre-processing on the input data, such as removing punctuation. 

Figure \ref{fig:nb4000_twtok} shows the most definitive features of a model trained on 4000 tweets, where each tweet was tokenised using \texttt{TweetTokenizer}.  

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{NB4000_tw_tokenizer}
    \caption{Most definitive feature of a Naive Bayes Model, using TweetTokenizer}
    \label{fig:nb4000_twtok}
\end{figure}

Later the default methods of obtaining feature vectors were replaced by more efficient methods which perform tokenisation and extracting unigrams from the list of tokens.

\paragraph{Support Vector Machine}\mbox{}\\
The Support Vector Machine instance is based on the \texttt{LinearSVC} implementation by the \texttt{sklearn} library. 
Ultimately it was the fastest one to train and the most precise one on terms of the results. The model would accept a large number of tweets as a training set. Empirically, the training size of 300 000 tweets was found to be the best for the purpose of training this sentiment analysis model. The model is used later throughout the project.

\paragraph{Other classifiers}
The other two classifiers available in the NLTK library are the Maximum Entropy Classifier and the Decision Tree Classifier.

Unlike the Naive Bayes model, the Maximum Entropy model does not make the independence assumption. From all the models that fit the training data, the Maximum Entropy approach selects the one which has the largest entropy. Intuitively, the model should perform well on the tasks involving Natural Language Processing. However, its current implementation in NLTK did not produce good results. The model was too slow during training and was not memory-efficient (would constantly cause the program to run out of memory). At the same time, the existing SVM and Naive Bayes classifiers performed on a high enough level, so it was decided not to use the Maximum Entropy model. Finally, the results received from that model were significantly lower than the ones returned by the previous models. With 6000 tweets as a training set, the final accuracy was 63\%. 

The Decision Tree classifier was the slowest yet. The execution time of training a model on just 1800 tweets exceeded twelve minutes and even though it produced a high accuracy of 80\%, the result was comparable to the one produced by the previous models, so it was decided not to use it either.

\paragraph{Resulting platform}\mbox{}\\
The resulting application that performs the sentiment analysis training can be seen as a platform. The main method accepts a number of parameters such as the name of the classifier to use, how many items there should be in an n-gram, the length of training and testing sets, and the name of the feature extractor, because several test ones were written throughout the project. The program can train four different classifiers on custom-defined training and testing data, use three different feature extractors (a simple n-gram extractor that does almost no pre-processing, a noun phrase extractor that tries extracting noun phrases using library methods --- rather unsuccessfully --- and the pre-processing extractor, the most developed feature extractor that does all of pre-processing described above and then extracts n-grams ). This allows to create and compare different models, makes the application scalable and maintainable. It could later be used in other Natural Language Processing-related work.

%=======
\section{Topic Modelling}
\label{sec:tm}

twitter has limits for tweets
https://dev.twitter.com/rest/public/rate-limits


Getting keywords:

http://www.niemanlab.org/2015/07/the-new-york-times-built-a-robot-to-help-making-article-tagging-easier/
article tagging 

Google AdWords:
not relevant, can see in googleads.png

GoogleCorrelate:
not too good, but could work, I guess
googlecorrelation.png 


Google trends seems the best

synonyms, stemming etc, bag of words!!!!

