\let\textcircled=\pgftextcircled
\chapter{Context Survey}
\label{chap:context-survey}

\initial{T}he problem of computers and humans understanding each other has existed ever since the very first, non-digital, computers. Even though computers are typically much better at some tasks than humans, understanding the human speech is still remaining an unsolved task fro Computer Science. 

The history of Machine Translation could be traced to philosophical sources from as early as 17th century, such as the ideas of Descartes and Leibniz about the so-called `universal' languages. Descartes proposed cataloguing all the possible concepts and `elements of human imagination' \cite{descartes}. Leibniz came up with an idea of the `Characteristica universalis', a formal language which would be capable of expressing not only scientific statements but also the metaphysical concepts \cite{leibniz}. 

Later in the course of history, artificial languages such as Esperanto and Interlingua appeared. Esperanto was constructed earlier, in 1880s. Its author, Ludwik Zamenhof mentioned that his main goals for creating the language were to make it easy to learn for people of as many nationalities as possible and to ``find some means of overcoming the natural indifference of mankind" \cite{zamenhof1911international}. Interlingua is another auxiliary language, however, it was created later, in 1940s. Its goals differ significantly from the goals of the inventors of Esperanto. With Interlingua, the main idea behind the language was to create a naturalistic language (as opposed to Esperanto which was systematic) that would deny the naive idea that a language is simply a tool and would promote an understanding that a language represents a culture \cite{gode1951interlingua}.

An interesting pattern could noticed at this point in history: simultaneously with developing the first digital computers and, hence, the first programming languages, the humanity was working on the so-called auxiliary languages, the main purpose of which was to help people from different parts of the world understand each other better. The first computers, thus, could be seen as another `nation' that had to be understood by everyone else and that had to be taught to understand humans, too. At that time, with the creation of the first digital computers and Alan Turing's publication of his famous article ``Computing Machinery and Intelligence" \cite{turing1950computing}, the two problems could be united into one --- a meta-problem of understanding different ways of communication.

It is often considered that the history of the Natural Language Processing starts with George Artsrouni's patent (received in 1933) for his ``electronic brain" which was supposed to be able to translate the input text into one of the three languages pre-programmed for the machine \cite{hutchins2004two}. The machine was based on a simple automatic bilingual dictionary and used paper tape. 

In a way, Artsrouni's patent defined the direction of the machine translation-related research for the next decades and resulted in the famous Georgetown-IBM experiment, the results of which were published in 1954 \cite{dostert1955georgetown}. The experiment demonstrated translation of approximately 60 sentences from Russian to English mainly using the lexicographical approach \cite{hutchins2004georgetown}. The system had six grammar rules and a vocabulary of around 250 lexical items. The experiment was deemed a huge success by media, and it was promised that the machine translation problem would be solved ``within three to five years" \cite{plumb1954russian}. However, the problem of machine translation still remains unsolved. Later analysis showed the disadvantages and limitations of using lexicographical approach \cite{garvin1968georgetown}. The approach did not try to go into the semantics of the sentence or distinguish between different parts of speech, or count for the fact that the sentences in different languages are normally constructed in different ways. Most of the sentences presented during the experiment were constructed in the same way in English and Russian. However, this is not always true, and the system would not have been able to translate sentences which could not be translated word by word into English. 

Gradually, machine translation was deemed unsuccessful, due to the limitations and the irregularities of the natural languages. The classic joke about the translation of ``The spirit is willing but the flesh is weak" into ``The vodka is good but the meat is rotten" appeared \cite{hutchins1995whisky}. It is important to note that most of these jokes are merely myths, however, they demonstrate the limitations of mechanical translation without an attempt to recognise the context and, ultimately, the meaning of the text.

Despite the loss of interest in machine translation after the lack of progress and success in the area, the development of research in Natural Language Processing continued. During 1980s, a new wave of research started, which looked into using machine learning for computational linguistics. Machine learning was not possible before that due to the limitations of processing power, however, ever since the 1980s, a big part of the research in Natural Language Processing is based on statistical supervised approach \cite{mcqueen1995applying}, \cite{bergsma2010creating}. The constant increase in computational power due to the Moore's Law allows to process large amounts of data and use it to construct complex language models which could be applied in vast number of areas, such as marketing \cite{goldberg1999automated}, medicine \cite{demner2009can}, and automated translation \cite{brants2007large}.

The biggest advantage of this approach is the fact that large amount of data can be processed quickly. However, in order to use supervised learning algorithms, all the training data has to be labelled manually. This is often proves to be a tedious, if not impossible, task. Only large companies and research groups can afford to perform manual labelling on a scale which allows to then train complex models. For example, the SNLI corpus created by the Stanford Natural Language Processing Group has 570 000 manually labelled sentences \cite{ferrero2017compilig}. In some cases, this task can be outsourced to the general public, and platforms such as Amazon's Mechanical Turk are becoming increasingly popular \cite{callison2010creating}. However, often deep linguistic knowledge is necessary to perform the labelling correctly. At the same time, some research suggests that, when certain quality and bias control are introduced properly, crowdsourcing can have immense positive impact on Natural Language Processing research \cite{sabou2012crowdsourcing}, \cite{snow2008cheap}.

The latest research in Natural Language Processing has turned to the semi-supervised and unsupervised methods of learning \cite{liang2005semi}, \cite{turian2010word}. This allows to avoid having to label the training data, however, is much more difficult than supervised learning, and, therefore, the current results are generally less accurate than those given by the supervised models \cite{lapata2005web}.

The current project focuses mostly on text classification in terms of sentiment and topic. 

\section{Sentiment Analysis}
\label{sec:sentiment_analysis}

Sentiment analysis is an area of Natural Language Processing that has a goal of determining the attitude or an emotion expressed in a text. The common existing sentiment analysis systems try to detect how positive or negative the text is \cite{yi2003sentiment}. The ultimate goal would be to determine the sentiment of the given text on a much wider spectre (such as sadness, anger, envy etc.), however, since this field is still quite young, even the problem of determining whether the text is positive or negative proves to be a complicated problem already. The simplest models look at just two classes: positive and negative (as Booleans). More complicated systems introduce the third class - neutral. Another approach expresses the sentiment as a score with clearly defined lower and higher limits, where the lowest score means the most negative text and the highest score stands for the most positive text.

The approaches used to construct sentiment analysis models can roughly be divided into three groups: knowledge-based techniques, statistical approaches, and hybrid approaches. 

The knowledge-based approaches use different ways to store the existing data (called the knowledge base, hence the name of the approach) about the subject, and check for the presence of the words describing that data \cite{chaumartin2007upar7}. The data can be expressed in a variety of ways, for example, with a simple dictionary or with a general-purpose semantic knowledge bases, or on a concept level \cite{cambria2013knowledge}. 

The statistical approaches employ machine learning techniques to construct the models. The researcher needs to extract the correct features and give them to a classifier to learn \cite{cambria2013statistical}. This approach allows to work with large amounts of data. Different techniques are used in this approach, such as the ``bag-of-words" approach, as well as the well known classifiers such as the Support Vector Machines and Naive-Bayes classifiers \cite{mullen2004sentiment}, \cite{tan2009adapting}.

The hybrid approaches aim to combine the knowledge-based and statistical approaches \cite{ghiassi2013twitter}. 

Overall, the state-of-the-art statistical models claim to reach accuracy around 80\%, whereas the hybrid ones reach 84\% \cite{thakkar2015approaches}. This number might not seem very high to some, however, in fact, accuracy around 80\% means that the model is as good in classifying text as a human. This is due to the fact that even people do not always agree when manually labelling text as positive or negative. In fact, research shows that the inter-rater reliability among people labelling polarity of text is around 79\%. 

\section{Topic modelling}
\label{sec:topic_modelling}

\section{Twitter Data Analysis}
\label{sec:twitter_analysis}

\section{Text classification}
\label{sec:text_classification}

 
 