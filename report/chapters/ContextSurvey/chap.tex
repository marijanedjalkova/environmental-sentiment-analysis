\let\textcircled=\pgftextcircled
\chapter{Context Survey}
\label{chap:context-survey}

\initial{T}he problem of computers and humans understanding each other has existed ever since the very first, non-digital, computers. Even though computers are typically much better at some tasks than humans, understanding the human speech is still remaining an unsolved task fro Computer Science. 

The history of Machine Translation could be traced to philosophical sources from as early as 17th century, such as the ideas of Descartes and Leibniz about the so-called `universal' languages. Descartes proposed cataloguing all the possible concepts and `elements of human imagination' \cite{descartes}. Leibniz came up with an idea of the 'Characteristica universalis', a formal language which would be capable of expressing not only scientific statements but also the metaphysical concepts \cite{leibniz}. 

Later in the course of history, artificial languages such as Esperanto and Interlingua appeared. Esperanto was constructed earlier, in 1880s. Its author, Ludwik Zamenhof mentioned that his main goals for creating the language were to make it easy to learn for people of as many nationalities as possible and to ``find some means of overcoming the natural indifference of mankind" \cite{zamenhof1911international}. Interlingua is another auxiliary language, however, it was created later, in 1940s. Its goals differ significantly from the goals of the inventors of Esperanto. With Interlingua, the main idea behind the language was to create a naturalistic language (as opposed to Esperanto which was systematic) that would deny the naive idea that a language is simply a tool and would promote an understanding that a language represents a culture \cite{gode1951interlingua}.

An interesting pattern could noticed at this point in history: simultaneously with developing the first digital computers and, hence, the first programming languages, the humanity was working on the so-called auxiliary languages, the main purpose of which was to help people from different parts of the world understand each other better. The first computers, thus, could be seen as another `nation' that had to be understood by everyone else and that had to be taught to understand humans, too. At that time, with the creation of the first digital computers and Alan Turing's publication of his famous article ``Computing Machinery and Intelligence" \cite{turing1950computing}, the two problems could be united into one --- a meta-problem of understanding different ways of communication.

It is often considered that the history of the Natural Language Processing starts with George Artsrouni's patent (received in 1933) for his "electronic brain" which was supposed to be able to translate the input text into one of the three languages pre-programmed for the machine \cite{hutchins2004two}. The machine was based on a simple automatic bilingual dictionary and used paper tape. 

In a way, Artsrouni's patent defined the direction of the machine translation-related research for the next decades and resulted in the famous Georgetown-IBM experiment, the results of which were published in 1954 \cite{dostert1955georgetown}. The experiment demonstrated translation of approximately 60 sentences from Russian to English mainly using the lexicographical approach \cite{hutchins2004georgetown}. The system had six grammar rules and a vocabulary of around 250 lexical items. The experiment was deemed a huge success by media, and it was promised that the machine translation problem would be solved ``within three to five years" \cite{plumb1954russian}. However, the problem of machine translation still remains unsolved. Later analysis showed the disadvantages and limitations of using lexicographical approach \cite{garvin1968georgetown}. The approach did not try to go into the semantics of the sentence or distinguish between different parts of speech, or count for the fact that the sentences in different languages are normally constructed in different ways. Most of the sentences presented during the experiment were constructed in the same way in English and Russian. However, this is not always true, and the system would not have been able to translate sentences which could not be translated word by word into English. 

Gradually, machine translation was deemed unsuccessful, due to the limitations and the irregularities of the natural languages. The classic joke about the translation of ``The spirit is willing but the flesh is weak" into ``The vodka is good but the meat is rotten" appeared \cite{hutchins1995whisky}. It is important to note that most of these jokes are merely myths, however, they demonstrate the limitations of mechanical translation without an attempt to recognise the context and, ultimately, the meaning of the text.

Despite the loss of interest in machine translation after the lack of progress and success in the area, the development of research in Natural Language Processing continued. During 1980s, a new wave of research started, which looked into using machine learning for computational linguistics. Machine learning was not possible before that due to the limitations of processing power. Ever since the 1980s, a big part of the research in Natural Language Processing is based on statistical supervised approach. The constant increase in computational power due to the Moore's Law allows to process large amounts of data and use it to construct complex language models which could be applied in vast number of areas.

The biggest advantage of this approach is the fact that large amount of data can be processed quickly. However, in order to use supervised learning algorithms, all the training data has to be labelled manually. This is often proves to be a tedious, if not impossible, task. Only large companies and research groups can afford to perform manual labelling on a scale which allows to then train complex models. In some cases, this task can be outsourced to the general public. However, often quite deep linguistic knowledge is necessary to perform the labelling correctly. 

The latest research in Natural Language Processing has turned to the semi-supervised and unsupervised methods of learning. This allows to avoid having to label the training data, however, is much more difficult than supervised learning, and, therefore, the current results are much less accurate than those given by the supervised models.


\section{Sentiment Analysis}
\label{sec:sentiment_analysis}

\section{Topic modelling}
\label{sec:topic_modelling}

\section{Twitter Data Analysis}
\label{sec:twitter_analysis}

\section{Text classification}
\label{sec:text_classification}

 
 