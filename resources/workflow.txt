0. [Report] -> initial findings, existing research
1. Try polarity on St Andrews data:
	use twitter API to get tweets about St Andrews - [DONE]. No way to use coordinates because most of people of not use them. Maybe
	use user.location ? Not sensible to try and locate the tweets. Only about 20% of tweets are geotagged. It is possible to estimate the location without tagging, however, it's quite complicated and has legal issues. References in the text file.
	use existing polarity tools to make analysis.
		- knowledge-based: 
			- TextBlob - around 60%.
		- statistical: 
			[Used Sentiment-Dataset: 50.05% pos, 49.9% neg]
			Sentiment-Dataset consists of tweets from 2 sources, intermingled. The ones from Sentiment140 are okay, but the ones from Kaggle seem to be about movie reviews, so they migth be skewing data. Should not train or test using Kaggle data.
			- My own very simple classifier: 62% using word_tokenize. Realised that this does not tokenize words correctly.
			Examples can be seen in most_pos_neg.jpg. NLTK provides own TweetTokenizer. Using that, the structure of the
			dictionary changed. The most defining features changed, can be seen in most_pos_neg_tw_tokenizer.jpg. The
			accuracy, however, dropped to 60%.
			- Naive Bayes - 40-70% (~ 1000 tweet training set, slow -> 46%). Very slow and takes
			a lot of memory. With TweetTokenizer: cannot do 8k training set (memory error). On 4k training set shows 79%
			accuracy. Most definitive features can be found in NB4000_tw_tokenizer.jpg. On 6k - 64% accuracy. 5k -> 77%. 3k
			-> 88%. 3.5k->80%. 2k->69%. So, best is 3k for training data. After removing Kaggle data, 88.88% on 3k training.
			Deleting urls and mentions does not affect the accuracy at all. Neither does replacing them with tags. 
			Removing stopwords reduces the accuracy a bit, to 86.86%. 
			- Bigram Naive Bayes - above 70% accuracy, much faster than the default one and can train on much more data.
			Tested on preprocessed tweets. Most popular bigrams can be seen in most_definitive_bigrams.jpg. Trigrams lose in
			accuracy (about 60%) a bit, the top trigrams include MENTIONs. Important to note that tweets are stripped off
			the stopwords, so bigrams could cover more than just 2 words. Changed the classifier to be ngram based,
			depending on the parameter. Changing text to lowercase except CAPS dropped the accuracy a tiny bit - from 74% to
			70% in some cases, but forced it up a little in some cases, too. 
			- MaxEntClassifier on bigrams was very slow, does 100 iterations and, after training on 6k tweets,
			produced 63% accuracy. 1k->50%. Might try more later. 
			- DecisionTreeClassifier on bigrams: the slowest yet. 800 training, 20 testing: 7 minutes,
			75% accuracy. Seem to be faster on unigrams. on 200 training, 20 testing 65%. (800, 20): 70%. 
			(1800, 20), 12 mins - 80%. 2k testing - 75%. (2700, 20), 36mins - 80%. Can't do more - exceeds time limits. 
			
[DATASET ISSUE] So after trying the model on the actual tweets, I can say that the dataset is not very convenient and does not represent what is going on on Twitter. Here (http://help.sentiment140.com/for-students/) it says that the tweets were generated automatically and that there is an assumption that positive smileys are used in the positive tweets, and vice versa. I can't agree with it...
After trying on 150k training set, bigram Naive Bayes seems to make more sense.

[NOUN PHRASE and other EXTRACTORS]: Textblob provides Fast_NP_extractor and ConllExtractor. Should try them as features. The
noun phrase extractors are not working very well. They do not extract enough. For example, from "what a lovely day it is right
now, going for a walk. I hope to see a lot of sun" only "lovely day" and "what lovely day" are being extracted by each of them
corespondingly. They are also very slow and taking several seconds for every single tweet. This is probably unusable unless
other extractors can be found.

Basic extractor seems to be much slower than the unigram extractor. There is also a contains_extractor. 
			- LinearSVC cassifier on 300k training set is reasonably fast and gives good results. I think I will go with it.
	
 
	come to a conclusion - do I need to create something of my own or are the existing tools good enough? - I don't need to create own classifier, but I wil need to create own feature extractor to customize the model. Also there is a paper that says that knowledge-based classifiers are better, so I should try them. 
	[Report] -> findings.
2. Try topic modeling on London data:
	use Twitter API to get tweets about London - [DONE].
	use existing topic modeling tools to make analysis
	[Report] -> conclusion
3. Try combining topic modelling and sentiment analysis. There are papers on that.

