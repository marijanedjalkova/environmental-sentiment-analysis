0. [Report] -> initial findings, existing research
1. Try polarity on St Andrews data:
	use twitter API to get tweets about St Andrews - [DONE]. No way to use coordinates because most of people of not use them. Maybe
	use user.location ? Not sensible to try and locate the tweets. Only about 20% of tweets are geotagged. It is possible to estimate the location without tagging, however, it's quite complicated and has legal issues. References in the text file.
	use existing polarity tools to make analysis.
		- knowledge-based: 
			- TextBlob - around 60%.
		- statistical: 
			[Used Sentiment-Dataset: 50.05% pos, 49.9% neg] - from thinknook.com
			Sentiment-Dataset consists of tweets from 2 sources, intermingled. The ones from Sentiment140 are okay, but the ones from Kaggle seem to be about movie reviews, so they migth be skewing data. Should not train or test using Kaggle data.
			- My own very simple classifier: 62% using word_tokenize. Realised that this does not tokenize words correctly.
			Examples can be seen in most_pos_neg.jpg. NLTK provides own TweetTokenizer. Using that, the structure of the
			dictionary changed. The most defining features changed, can be seen in most_pos_neg_tw_tokenizer.jpg. The
			accuracy, however, dropped to 60%.
			- Naive Bayes - 40-70% (~ 1000 tweet training set, slow -> 46%). Very slow and takes
			a lot of memory. With TweetTokenizer: cannot do 8k training set (memory error). On 4k training set shows 79%
			accuracy. Most definitive features can be found in NB4000_tw_tokenizer.jpg. On 6k - 64% accuracy. 5k -> 77%. 3k
			-> 88%. 3.5k->80%. 2k->69%. So, best is 3k for training data. After removing Kaggle data, 88.88% on 3k training.
			Deleting urls and mentions does not affect the accuracy at all. Neither does replacing them with tags. 
			Removing stopwords reduces the accuracy a bit, to 86.86%. 
			- Bigram Naive Bayes - above 70% accuracy, much faster than the default one and can train on much more data.
			Tested on preprocessed tweets. Most popular bigrams can be seen in most_definitive_bigrams.jpg. Trigrams lose in
			accuracy (about 60%) a bit, the top trigrams include MENTIONs. Important to note that tweets are stripped off
			the stopwords, so bigrams could cover more than just 2 words. Changed the classifier to be ngram based,
			depending on the parameter. Changing text to lowercase except CAPS dropped the accuracy a tiny bit - from 74% to
			70% in some cases, but forced it up a little in some cases, too. 
			- MaxEntClassifier on bigrams was very slow, does 100 iterations and, after training on 6k tweets,
			produced 63% accuracy. 1k->50%. Might try more later. 
			- DecisionTreeClassifier on bigrams: the slowest yet. 800 training, 20 testing: 7 minutes,
			75% accuracy. Seem to be faster on unigrams. on 200 training, 20 testing 65%. (800, 20): 70%. 
			(1800, 20), 12 mins - 80%. 2k testing - 75%. (2700, 20), 36mins - 80%. Can't do more - exceeds time limits. 
			
[DATASET ISSUE] So after trying the model on the actual tweets, I can say that the dataset from thinknook.com is not very convenient and does not represent what is going on on Twitter. Here (http://help.sentiment140.com/for-students/) it says that the tweets were generated automatically and that there is an assumption that positive smileys are used in the positive tweets, and vice versa. I can't agree with it...
After trying on 150k training set, bigram Naive Bayes seems to make more sense.

[NOUN PHRASE and other EXTRACTORS]: Textblob provides Fast_NP_extractor and ConllExtractor. Should try them as features. The
noun phrase extractors are not working very well. They do not extract enough. For example, from "what a lovely day it is right
now, going for a walk. I hope to see a lot of sun" only "lovely day" and "what lovely day" are being extracted by each of them
corespondingly. They are also very slow and taking several seconds for every single tweet. This is probably unusable unless
other extractors can be found.

Basic extractor seems to be much slower than the unigram extractor. There is also a contains_extractor. 
			- LinearSVC cassifier on 300k training set extracting unigrams is reasonably fast and gives good results. I think I will go with it. Same thing extracting bigrams makes much less sense, so should stick with unigrams.
 
	come to a conclusion - do I need to create something of my own or are the existing tools good enough? - I don't need to create own classifier, but I wil need to create own feature extractor to customize the model. Also there is a paper that says that knowledge-based classifiers are better, so I should try them.
	At this point some sort of a sentiment analysis tool works. Some things to consider at this point are normalizing slang, for example. Sopme tweets have Scottish slang, so it should ideally be normalized. Also, smileys need attention, as well as irony and sarcasm. 
	
	To validate the model somehow, tested against TextBlob's offline sentiment analysis model. (any other tool is not
	accessible online, for some reason). Textblob's sentiment is measured between -1 and 1, so it is difficult to decide
	what to do with neutral tweets. The model guesses right about 67% of the 65 tweets (44), and the neutral ones are not
	counted within those 67%. Of 1000 tweets guesses 40%, and another 40% are neutral.
	However, some of the judgements the Textblob's sentiment tool makes are incorrect. For example, this
	tweet: "The rugby club will be open on Saturday (11/03/2017) for 6 nations game \n https://t.co/mtT2Dgk9Vz", which was
	considered negative by Textblob and positive by my classifier. Also this one: "Late night studio sesh.
	#stirlingcityradio #morethanahobby #wayoflife @ Grangemouth https://t.co/uw8mSdGwJg". 

	Previously sentdex.com was available for these calls but it is inaccessible now. 
	Attempted to compare to sentiment140. Compared on the basis of 520 tweets. 53 were guessed correctly, however, 453 were 
	neutral, so it is hard to say whether the model is good. Ideally would try to train a trinary model. 2 tweets could not
	be processed by sentiment140 due to weird characters. Only 11 tweets were guessed completely incorrectly. Of them 11
	only 5 were unique. This makes me think that the process of searching through tweets is not perfect.
	
	Tried another dataset, the actual dataset from sentiment140 (http://help.sentiment140.com/for-students/), but there are
	no neutral tweets there either.
	
	[Report] -> findings.
2. Try topic modeling on London data:
	use Twitter API to get tweets about London - [DONE].
	use existing topic modeling tools to make analysis
	[Report] -> conclusion
3. Try combining topic modelling and sentiment analysis. There are papers on that.

