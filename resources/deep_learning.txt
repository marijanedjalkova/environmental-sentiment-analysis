https://github.com/oxford-cs-deepnlp-2017/lectures - Oxford DeepMind lectures

    Count-based methods:
    - Define a basis vocabulary C of context words. (e.g. {bit, cute, furry, loud, miaowed, purred, ran, small}.)
    - Define a word window size w. (e.g. 1)
    - Count the basis vocabulary words occurring w words to the left or right of each instance of a target word (e.g. cat) in the corpus.
      (e.g. cute kitten purred)
    - Form a vector representation of the target word based on these counts. (e.g. kitten = [0, 1, 0, 0, 1, 1, 0, 1])
      (look at the vocabulary and put 1/0 for every word based on whether it occurs near the target word)
    - use inner product or cosine as similarity kernel (e.g. sim(kitten, cat) = cosine(kitten, cat) = 0.58)
    NOTE: normalize

    Neural Embedding Models:
    - produce embedding matrix (vocab x content) by putting vectors one on top of other
    - (onehot?)
    - NOTE: target word predicts context words (? can be used with Grangemouth ?). embed target word. project into vocabulary. softmax.
    - benefits: easy to learn, highly parallel problem

    Task-based Embedding learning:
    - get input features to a neural network from words
    - We can therefore directly train embeddings jointly with the parameters of the
      network which uses them.
    - General intuition: learn to classify/predict/generate based on features, but also
      the features themselves.
    - BoW (Bag of Words) Classifiers:
        - want to classify sentences based on a variable number of word respresentations
        - solution: bag of vectors
        - simple to implement and train
        - no notion of words in context
        - NOTE: Used for sentiment analysis (e.g. tweets)!!!

        Learning and re-using word vectors is a form of transfer learning. It is particularly
        useful if you have little task-specific training data, or poor coverage of the
        vocabulary (in which case you might not want to fine-tune embeddings).

Neural Language Models:
  - Feed forward model: x -> h -> y^ : h = g(Vx+c), y^ = Wh+b
    - 3gram: h = g(V[w(n-1);w(n-2)]+c), y^ = softmax(Wh+b); softmax(u)i = exp(u(i)) / (sum(j)(exp(u(j))))
        w(i) are onehot vectors, y^ - distributions

  - Recurrent Neural network:
    - h = g( V[x(n); h(n-1)] + c), y^(n) = Wh(n) + b
    - can represent unbounded dependencies
    - compress histories of words into a fixed zies hidden vector
    - hard to learn
    - increasing size of hidden layer increases computation and memory quadratically

TEXT CLASSIFICATION (*lecture 5*):

    - Generative Models: Model the distribution of individual classes and place
probabilities over both observed data and hidden variables (e.g. N-grams, HMMs, Naive Bayes)

      * Naive Bayes *
          P(c|d) = P(c)P(d|c) / P(d). Can ignore the denominator because it's independent of c.
          Can simplify to P(c) *  ÐŸ(1<=i<=n(d)) P(t(i)|c)
          P(c) = D(c)/D
          P(t|c) = ... (** see formula in slides)
          P(ti|c) = P(tj|c) <- independent of token position -> NAIVE
          - simple, fast, interpretable; BoW
          - independence assumptions, sentence structure not taken into account

    - Discriminative models: Learn boundaries between classes. Take data as given and put
probability over the hidden structure given the data. (e.g. logistic regression, SVMs, Max entropy models)

      * Logistic regression * ( NOTE: maxent in nltk)
        - If we only need to classify text, do not need full power of generative model
        - can be binary or multinomial
        - binary and general functions can be simp-lified to logistic and softmax functions.
        - * SOFTMAX *: takes output of K distinct linear functions and returns a probability distribution over those outputs

      * Recurrent Neural Networks * can also be used.
